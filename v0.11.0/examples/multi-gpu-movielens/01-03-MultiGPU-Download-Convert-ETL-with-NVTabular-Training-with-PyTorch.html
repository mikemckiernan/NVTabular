<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-GPU with MovieLens: ETL and Training &mdash; NVTabular 2021 documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/NVTabular/main/examples/multi-gpu-movielens/01-03-MultiGPU-Download-Convert-ETL-with-NVTabular-Training-with-PyTorch.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-GPU Scaling in NVTabular with Dask" href="../multi-gpu-toy-example/multi-gpu_dask.html" />
    <link rel="prev" title="Multi-GPU with MovieLens: ETL and Training" href="01-03-MultiGPU-Download-Convert-ETL-with-NVTabular-Training-with-TensorFlow.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> NVTabular
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Accelerated Training</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Example Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#structure">Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#available-example-notebooks">Available Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#running-the-example-notebooks">Running the Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting-started-movielens/index.html">Getting Started with MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced-ops-outbrain/index.html">Advanced Ops with Outbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scaling-criteo/index.html">Scaling to Large Datasets with Criteo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tabular-data-rossmann/index.html">Applying Techniques to Rossmann Stores Data</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Multi-GPU Example Notebooks</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="01-03-MultiGPU-Download-Convert-ETL-with-NVTabular-Training-with-TensorFlow.html">Multi-GPU with MovieLens: ETL and Training</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Multi-GPU with MovieLens: ETL and Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../multi-gpu-toy-example/multi-gpu_dask.html">Multi-GPU Scaling in NVTabular with Dask</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../winning-solution-recsys2020-twitter/01-02-04-Download-Convert-ETL-with-NVTabular-Training-with-XGBoost.html">Winning Solution of the RecSys2020 Competition</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources/index.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NVTabular</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">NVTabular Example Notebooks</a></li>
          <li class="breadcrumb-item"><a href="index.html">Multi-GPU Example Notebooks</a></li>
      <li class="breadcrumb-item active">Multi-GPU with MovieLens: ETL and Training</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Copyright 2021 NVIDIA Corporation. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
</pre></div>
</div>
</div>
</div>
<img alt="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" src="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" />
<div class="section" id="multi-gpu-with-movielens-etl-and-training">
<h1>Multi-GPU with MovieLens: ETL and Training<a class="headerlink" href="#multi-gpu-with-movielens-etl-and-training" title="Permalink to this headline"></a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>NVIDIA Merlin is a open source framework to accelerate and scale end-to-end recommender system pipelines on GPU. In this notebook, we use NVTabular, Merlin’s ETL component, to scale feature engineering and pre-processing to multiple GPUs and then perform data-parallel distributed training of a neural network on multiple GPUs with PyTorch, <a class="reference external" href="https://horovod.readthedocs.io/en/stable/">Horovod</a>, and <a class="reference external" href="https://developer.nvidia.com/nccl">NCCL</a>.</p>
<p>The pre-requisites for this notebook are to be familiar with NVTabular and its API:</p>
<ul class="simple">
<li><p>You can read more about NVTabular, its API and specialized dataloaders in <a class="reference internal" href="../getting-started-movielens/index.html"><span class="xref myst">Getting Started with Movielens notebooks</span></a>.</p></li>
<li><p>You can read more about scaling NVTabular ETL in <a class="reference internal" href="../scaling-criteo/index.html"><span class="xref myst">Scaling Criteo notebooks</span></a>.</p></li>
</ul>
<p><strong>In this notebook, we will focus only on the new information related to multi-GPU training, so please check out the other notebooks first (if you haven’t already.)</strong></p>
<div class="section" id="learning-objectives">
<h3>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline"></a></h3>
<p>In this notebook, we learn how to scale ETL and deep learning taining to multiple GPUs</p>
<ul class="simple">
<li><p>Learn to use larger than GPU/host memory datasets for ETL and training</p></li>
<li><p>Use multi-GPU or multi node for ETL with NVTabular</p></li>
<li><p>Use NVTabular dataloader to accelerate PyTorch pipelines</p></li>
<li><p>Scale PyTorch training with Horovod</p></li>
</ul>
</div>
<div class="section" id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline"></a></h3>
<p>In this notebook, we use the <a class="reference external" href="https://grouplens.org/datasets/movielens/25m/">MovieLens25M</a> dataset. It is popular for recommender systems and is used in academic publications. The dataset contains 25M movie ratings for 62,000 movies given by 162,000 users. Many projects use only the user/item/rating information of MovieLens, but the original dataset provides metadata for the movies, as well.</p>
<p>Note: We are using the MovieLens 25M dataset in this example for simplicity, although the dataset is not large enough to require multi-GPU training. However, the functionality demonstrated in this notebook can be easily extended to scale recommender pipelines for larger datasets in the same way.</p>
</div>
<div class="section" id="tools">
<h3>Tools<a class="headerlink" href="#tools" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://horovod.readthedocs.io/en/stable/">Horovod</a> is a distributed deep learning framework that provides tools for multi-GPU optimization.</p></li>
<li><p>The <a class="reference external" href="https://developer.nvidia.com/nccl">NVIDIA Collective Communication Library (NCCL)</a> provides the underlying GPU-based implementations of the <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#allgather">allgather</a> and <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#allreduce">allreduce</a> cross-GPU communication operations.</p></li>
</ul>
</div>
</div>
<div class="section" id="download-and-convert">
<h2>Download and Convert<a class="headerlink" href="#download-and-convert" title="Permalink to this headline"></a></h2>
<p>First, we will download and convert the dataset to Parquet. This section is based on <a class="reference internal" href="../getting-started-movielens/01-Download-Convert.html"><span class="doc std std-doc">01-Download-Convert.ipynb</span></a>.</p>
<div class="section" id="download">
<h3>Download<a class="headerlink" href="#download" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># External dependencies
import os
import pathlib

import cudf  # cuDF is an implementation of Pandas-like Dataframe on GPU

from nvtabular.utils import download_file

INPUT_DATA_DIR = os.environ.get(&quot;INPUT_DATA_DIR&quot;, &quot;~/nvt-examples/multigpu-movielens/data/&quot;)
BASE_DIR = pathlib.Path(INPUT_DATA_DIR).expanduser()
zip_path = pathlib.Path(BASE_DIR, &quot;ml-25m.zip&quot;)
download_file(
    &quot;http://files.grouplens.org/datasets/movielens/ml-25m.zip&quot;, zip_path, redownload=False
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>downloading ml-25m.zip: 262MB [01:11, 3.66MB/s]                                                                                             
unzipping files: 100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:09&lt;00:00,  1.16s/files]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="convert">
<h3>Convert<a class="headerlink" href="#convert" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>movies = cudf.read_csv(pathlib.Path(BASE_DIR, &quot;ml-25m&quot;, &quot;movies.csv&quot;))
movies[&quot;genres&quot;] = movies[&quot;genres&quot;].str.split(&quot;|&quot;)
movies = movies.drop(&quot;title&quot;, axis=1)
movies.to_parquet(pathlib.Path(BASE_DIR, &quot;ml-25m&quot;, &quot;movies_converted.parquet&quot;))
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="split-into-train-and-validation-datasets">
<h3>Split into train and validation datasets<a class="headerlink" href="#split-into-train-and-validation-datasets" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ratings = cudf.read_csv(pathlib.Path(BASE_DIR, &quot;ml-25m&quot;, &quot;ratings.csv&quot;))
ratings = ratings.drop(&quot;timestamp&quot;, axis=1)

# shuffle the dataset
ratings = ratings.sample(len(ratings), replace=False)
# split the train_df as training and validation data sets.
num_valid = int(len(ratings) * 0.2)
train = ratings[:-num_valid]
valid = ratings[-num_valid:]

train.to_parquet(pathlib.Path(BASE_DIR, &quot;train.parquet&quot;))
valid.to_parquet(pathlib.Path(BASE_DIR, &quot;valid.parquet&quot;))
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="etl-with-nvtabular">
<h2>ETL with NVTabular<a class="headerlink" href="#etl-with-nvtabular" title="Permalink to this headline"></a></h2>
<p>We finished downloading and converting the dataset. We will preprocess and engineer features with NVTabular on multiple GPUs. You can read more</p>
<ul class="simple">
<li><p>about NVTabular’s features and API in <a class="reference internal" href="../getting-started-movielens/02-ETL-with-NVTabular.html"><span class="doc std std-doc">getting-started-movielens/02-ETL-with-NVTabular.ipynb</span></a>.</p></li>
<li><p>scaling NVTabular ETL to multiple GPUs <a class="reference internal" href="../scaling-criteo/02-ETL-with-NVTabular.html"><span class="doc std std-doc">scaling-criteo/02-ETL-with-NVTabular.ipynb</span></a>.</p></li>
</ul>
<div class="section" id="deploy-a-distributed-dask-cluster">
<h3>Deploy a Distributed-Dask Cluster<a class="headerlink" href="#deploy-a-distributed-dask-cluster" title="Permalink to this headline"></a></h3>
<p>This section is based on <a class="reference internal" href="../scaling-criteo/02-ETL-with-NVTabular.html"><span class="doc std std-doc">scaling-criteo/02-ETL-with-NVTabular.ipynb</span></a> and <a class="reference internal" href="../multi-gpu-toy-example/multi-gpu_dask.html"><span class="doc std std-doc">multi-gpu-toy-example/multi-gpu_dask.ipynb</span></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Standard Libraries
import shutil

# External Dependencies
import cudf
from dask_cuda import LocalCUDACluster
from dask.distributed import Client
import rmm

# NVTabular
import nvtabular as nvt
from nvtabular.io import Shuffle
from nvtabular.utils import device_mem_size
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># define some information about where to get our data
input_path = pathlib.Path(BASE_DIR, &quot;converted&quot;, &quot;movielens&quot;)
dask_workdir = pathlib.Path(BASE_DIR, &quot;test_dask&quot;, &quot;workdir&quot;)
output_path = pathlib.Path(BASE_DIR, &quot;test_dask&quot;, &quot;output&quot;)
stats_path = pathlib.Path(BASE_DIR, &quot;test_dask&quot;, &quot;stats&quot;)

# Make sure we have a clean worker space for Dask
if pathlib.Path.is_dir(dask_workdir):
    shutil.rmtree(dask_workdir)
dask_workdir.mkdir(parents=True)

# Make sure we have a clean stats space for Dask
if pathlib.Path.is_dir(stats_path):
    shutil.rmtree(stats_path)
stats_path.mkdir(parents=True)

# Make sure we have a clean output path
if pathlib.Path.is_dir(output_path):
    shutil.rmtree(output_path)
output_path.mkdir(parents=True)

# Get device memory capacity
capacity = device_mem_size(kind=&quot;total&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Deploy a Single-Machine Multi-GPU Cluster
protocol = &quot;tcp&quot;  # &quot;tcp&quot; or &quot;ucx&quot;
visible_devices = &quot;0,1&quot;  # Delect devices to place workers
device_spill_frac = 0.5  # Spill GPU-Worker memory to host at this limit.
# Reduce if spilling fails to prevent
# device memory errors.
cluster = None  # (Optional) Specify existing scheduler port
if cluster is None:
    cluster = LocalCUDACluster(
        protocol=protocol,
        CUDA_VISIBLE_DEVICES=visible_devices,
        local_directory=dask_workdir,
        device_memory_limit=capacity * device_spill_frac,
    )

# Create the distributed client
client = Client(cluster)
client
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.8/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33621 instead
  warnings.warn(
distributed.preloading - INFO - Import preload module: dask_cuda.initialize
distributed.preloading - INFO - Import preload module: dask_cuda.initialize
</pre></div>
</div>
<div class="output text_html">
            <div>
                <div style="
                    width: 24px;
                    height: 24px;
                    background-color: #e1e1e1;
                    border: 3px solid #9D9D9D;
                    border-radius: 5px;
                    position: absolute;"> </div>
                <div style="margin-left: 48px;">
                    <h3 style="margin-bottom: 0px;">Client</h3>
                    <p style="color: #9D9D9D; margin-bottom: 0px;">Client-1186085c-5cc7-11ec-8176-54ab3a8c9e18</p>
                    <table style="width: 100%; text-align: left;">
                    
                <tr>
                    <td style="text-align: left;"><strong>Connection method:</strong> Cluster object</td>
                    <td style="text-align: left;"><strong>Cluster type:</strong> LocalCUDACluster</td>
                </tr>
                
                <tr>
                    <td style="text-align: left;">
                        <strong>Dashboard: </strong>
                        <a href="http://127.0.0.1:33621/status">http://127.0.0.1:33621/status</a>
                    </td>
                    <td style="text-align: left;"></td>
                </tr>
                
                    </table>
                    
                <details>
                <summary style="margin-bottom: 20px;"><h3 style="display: inline;">Cluster Info</h3></summary>
                
            <div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output">
                <div style="
                    width: 24px;
                    height: 24px;
                    background-color: #e1e1e1;
                    border: 3px solid #9D9D9D;
                    border-radius: 5px;
                    position: absolute;"> </div>
                <div style="margin-left: 48px;">
                    <h3 style="margin-bottom: 0px; margin-top: 0px;">LocalCUDACluster</h3>
                    <p style="color: #9D9D9D; margin-bottom: 0px;">d5de4e96</p>
                    <table style="width: 100%; text-align: left;">
                    
            <tr>
                <td style="text-align: left;"><strong>Status:</strong> running</td>
                <td style="text-align: left;"><strong>Using processes:</strong> True</td>
            </tr>
        
            <tr>
                <td style="text-align: left;">
                    <strong>Dashboard:</strong> <a href="http://127.0.0.1:33621/status">http://127.0.0.1:33621/status</a>
                </td>
                <td style="text-align: left;"><strong>Workers:</strong> 2</td>
            </tr>
            <tr>
                <td style="text-align: left;">
                    <strong>Total threads:</strong>
                    2
                </td>
                <td style="text-align: left;">
                    <strong>Total memory:</strong>
                    0.98 TiB
                </td>
            </tr>
        
                    </table>
                    <details>
                    <summary style="margin-bottom: 20px;"><h3 style="display: inline;">Scheduler Info</h3></summary>
                    
        <div style="">
            
            <div>
                <div style="
                    width: 24px;
                    height: 24px;
                    background-color: #FFF7E5;
                    border: 3px solid #FF6132;
                    border-radius: 5px;
                    position: absolute;"> </div>
                <div style="margin-left: 48px;">
                    <h3 style="margin-bottom: 0px;">Scheduler</h3>
                    <p style="color: #9D9D9D; margin-bottom: 0px;">Scheduler-2389d797-80ae-4c6d-a865-b6d3ca73515f</p>
                    <table style="width: 100%; text-align: left;">
                        <tr>
                            <td style="text-align: left;"><strong>Comm:</strong> tcp://127.0.0.1:43283</td>
                            <td style="text-align: left;"><strong>Workers:</strong> 2</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">
                                <strong>Dashboard:</strong> <a href="http://127.0.0.1:33621/status">http://127.0.0.1:33621/status</a>
                            </td>
                            <td style="text-align: left;">
                                <strong>Total threads:</strong>
                                2
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">
                                <strong>Started:</strong>
                                Just now
                            </td>
                            <td style="text-align: left;">
                                <strong>Total memory:</strong>
                                0.98 TiB
                            </td>
                        </tr>
                    </table>
                </div>
            </div>
        
            <details style="margin-left: 48px;">
            <summary style="margin-bottom: 20px;"><h3 style="display: inline;">Workers</h3></summary>
            
            <div style="margin-bottom: 20px;">
                <div style="width: 24px;
                            height: 24px;
                            background-color: #DBF5FF;
                            border: 3px solid #4CC9FF;
                            border-radius: 5px;
                            position: absolute;"> </div>
                <div style="margin-left: 48px;">
                <details>
                    <summary>
                        <h4 style="margin-bottom: 0px; display: inline;">Worker: 0</h4>
                    </summary>
                    <table style="width: 100%; text-align: left;">
                        <tr>
                            <td style="text-align: left;"><strong>Comm: </strong> tcp://127.0.0.1:36269</td>
                            <td style="text-align: left;"><strong>Total threads: </strong> 1</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">
                                <strong>Dashboard: </strong>
                                <a href="http://127.0.0.1:42147/status">http://127.0.0.1:42147/status</a>
                            </td>
                            <td style="text-align: left;">
                                <strong>Memory: </strong>
                                503.90 GiB
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: left;"><strong>Nanny: </strong> tcp://127.0.0.1:34769</td>
                            <td style="text-align: left;"></td>
                        </tr>
                        <tr>
                            <td colspan="2" style="text-align: left;">
                                <strong>Local directory: </strong>
                                /root/nvt-examples/multigpu-movielens/data/test_dask/workdir/dask-worker-space/worker-rpm3vlgv
                            </td>
                        </tr>
                        
                <tr>
                    <td style="text-align: left;">
                        <strong>GPU: </strong>Tesla V100-SXM2-32GB
                    </td>
                    <td style="text-align: left;">
                        <strong>GPU memory: </strong>
                        31.75 GiB
                    </td>
                </tr>
                
                        
                    </table>
                </details>
                </div>
            </div>
            
            <div style="margin-bottom: 20px;">
                <div style="width: 24px;
                            height: 24px;
                            background-color: #DBF5FF;
                            border: 3px solid #4CC9FF;
                            border-radius: 5px;
                            position: absolute;"> </div>
                <div style="margin-left: 48px;">
                <details>
                    <summary>
                        <h4 style="margin-bottom: 0px; display: inline;">Worker: 1</h4>
                    </summary>
                    <table style="width: 100%; text-align: left;">
                        <tr>
                            <td style="text-align: left;"><strong>Comm: </strong> tcp://127.0.0.1:34485</td>
                            <td style="text-align: left;"><strong>Total threads: </strong> 1</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">
                                <strong>Dashboard: </strong>
                                <a href="http://127.0.0.1:42123/status">http://127.0.0.1:42123/status</a>
                            </td>
                            <td style="text-align: left;">
                                <strong>Memory: </strong>
                                503.90 GiB
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: left;"><strong>Nanny: </strong> tcp://127.0.0.1:46881</td>
                            <td style="text-align: left;"></td>
                        </tr>
                        <tr>
                            <td colspan="2" style="text-align: left;">
                                <strong>Local directory: </strong>
                                /root/nvt-examples/multigpu-movielens/data/test_dask/workdir/dask-worker-space/worker-0lecf42d
                            </td>
                        </tr>
                        
                <tr>
                    <td style="text-align: left;">
                        <strong>GPU: </strong>Tesla V100-SXM2-32GB
                    </td>
                    <td style="text-align: left;">
                        <strong>GPU memory: </strong>
                        31.75 GiB
                    </td>
                </tr>
                
                        
                    </table>
                </details>
                </div>
            </div>
            
            </details>
        </div>
        
                    </details>
                </div>
            </div>
        
                </details>
                
                </div>
            </div>
        </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize RMM pool on ALL workers
def _rmm_pool():
    rmm.reinitialize(
        pool_allocator=True,
        initial_pool_size=None,  # Use default size
    )


client.run(_rmm_pool)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;tcp://127.0.0.1:34485&#39;: None, &#39;tcp://127.0.0.1:36269&#39;: None}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="defining-our-preprocessing-pipeline">
<h3>Defining our Preprocessing Pipeline<a class="headerlink" href="#defining-our-preprocessing-pipeline" title="Permalink to this headline"></a></h3>
<p>This subsection is based on <a class="reference internal" href="../getting-started-movielens/02-ETL-with-NVTabular.html"><span class="doc std std-doc">getting-started-movielens/02-ETL-with-NVTabular.ipynb</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>movies = cudf.read_parquet(pathlib.Path(BASE_DIR, &quot;ml-25m&quot;, &quot;movies_converted.parquet&quot;))
joined = [&quot;userId&quot;, &quot;movieId&quot;] &gt;&gt; nvt.ops.JoinExternal(movies, on=[&quot;movieId&quot;])
cat_features = joined &gt;&gt; nvt.ops.Categorify()
ratings = nvt.ColumnSelector([&quot;rating&quot;]) &gt;&gt; nvt.ops.LambdaOp(lambda col: (col &gt; 3).astype(&quot;int8&quot;))
output = cat_features + ratings
workflow = nvt.Workflow(output)
!rm -rf $BASE_DIR/train
!rm -rf $BASE_DIR/valid
train_iter = nvt.Dataset([str(pathlib.Path(BASE_DIR, &quot;train.parquet&quot;))], part_size=&quot;100MB&quot;)
valid_iter = nvt.Dataset([str(pathlib.Path(BASE_DIR, &quot;valid.parquet&quot;))], part_size=&quot;100MB&quot;)
workflow.fit(train_iter)
workflow.save(str(pathlib.Path(BASE_DIR, &quot;workflow&quot;)))
shuffle = Shuffle.PER_WORKER  # Shuffle algorithm
out_files_per_proc = 4  # Number of output files per worker
workflow.transform(train_iter).to_parquet(
    output_path=pathlib.Path(BASE_DIR, &quot;train&quot;),
    shuffle=shuffle,
    out_files_per_proc=out_files_per_proc,
)
workflow.transform(valid_iter).to_parquet(
    output_path=pathlib.Path(BASE_DIR, &quot;valid&quot;),
    shuffle=shuffle,
    out_files_per_proc=out_files_per_proc,
)

client.shutdown()
cluster.close()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.8/site-packages/distributed/worker.py:3801: UserWarning: Large object of size 3.80 MiB detected in task graph: 
  ([&lt;Node JoinExternal&gt;], &#39;read-parquet-b1cb9ccb4ca4 ...  1, 2], None)})
Consider scattering large objects ahead of time
with client.scatter to reduce scheduler burden and 
keep data on workers

    future = client.submit(func, big_data)    # bad

    big_future = client.scatter(big_data)     # good
    future = client.submit(func, big_future)  # good
  warnings.warn(
/nvtabular/nvtabular/io/dataset.py:868: UserWarning: Only created 2 files did not have enough
partitions to create 8 files.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="training-with-pytorch-on-multigpus">
<h2>Training with PyTorch on multiGPUs<a class="headerlink" href="#training-with-pytorch-on-multigpus" title="Permalink to this headline"></a></h2>
<p>In this section, we will train a PyTorch model with multi-GPU support. In the NVTabular v0.5 release, we added multi-GPU support for NVTabular dataloaders. We will modify the <a class="reference internal" href="../getting-started-movielens/03-Training-with-PyTorch.html"><span class="doc std std-doc">getting-started-movielens/03-Training-with-PyTorch.ipynb</span></a> to use multiple GPUs. Please review that notebook, if you have questions about the general functionality of the NVTabular dataloaders or the neural network architecture.</p>
<div class="section" id="nvtabular-dataloader-for-pytorch">
<h3>NVTabular dataloader for PyTorch<a class="headerlink" href="#nvtabular-dataloader-for-pytorch" title="Permalink to this headline"></a></h3>
<p>We’ve identified that the dataloader is one bottleneck in deep learning recommender systems when training pipelines with PyTorch. The normal PyTorch dataloaders cannot prepare the next training batches fast enough and therefore, the GPU is not fully utilized.</p>
<p>We developed a highly customized tabular dataloader for accelerating existing pipelines in PyTorch. In our experiments, we see a speed-up by 9x of the same training workflow with NVTabular dataloader. NVTabular dataloader’s features are:</p>
<ul class="simple">
<li><p>removing bottleneck of item-by-item dataloading</p></li>
<li><p>enabling larger than memory dataset by streaming from disk</p></li>
<li><p>reading data directly into GPU memory and remove CPU-GPU communication</p></li>
<li><p>preparing batch asynchronously in GPU to avoid CPU-GPU communication</p></li>
<li><p>supporting commonly used .parquet format</p></li>
<li><p>easy integration into existing PyTorch pipelines by using similar API</p></li>
<li><p><strong>supporting multi-GPU training with Horovod</strong></p></li>
</ul>
<p>You can find more information on the dataloaders in our <a class="reference external" href="https://medium.com/nvidia-merlin/training-deep-learning-based-recommender-systems-9x-faster-with-PyTorch-cc5a2572ea49">blogpost</a>.</p>
</div>
<div class="section" id="using-horovod-with-pytorch-and-nvtabular">
<h3>Using Horovod with PyTorch and NVTabular<a class="headerlink" href="#using-horovod-with-pytorch-and-nvtabular" title="Permalink to this headline"></a></h3>
<p>The training script below is based on <a class="reference internal" href="../getting-started-movielens/03-Training-with-PyTorch.html"><span class="doc std std-doc">getting-started-movielens/03-Training-with-PyTorch.ipynb</span></a>, with a few important changes:</p>
<ul class="simple">
<li><p>We provide several additional parameters to the <code class="docutils literal notranslate"><span class="pre">TorchAsyncItr</span></code> class, including the total number of workers <code class="docutils literal notranslate"><span class="pre">hvd.size()</span></code>, the current worker’s id number <code class="docutils literal notranslate"><span class="pre">hvd.rank()</span></code>, and a function for generating random seeds <code class="docutils literal notranslate"><span class="pre">seed_fn()</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TorchAsyncItr</span><span class="p">(</span>
        <span class="o">...</span>
        <span class="n">global_size</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
        <span class="n">global_rank</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">(),</span>
        <span class="n">seed_fn</span><span class="o">=</span><span class="n">seed_fn</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The seed function uses Horovod to collectively generate a random seed that’s shared by all workers so that they can each shuffle the dataset in a consistent way and select partitions to work on without overlap. The seed function is called by the dataloader during the shuffling process at the beginning of each epoch:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">seed_fn</span><span class="p">():</span>
        <span class="n">max_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span><span class="o">.</span><span class="n">max</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Generate a seed fragment</span>
        <span class="n">seed_fragment</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_rand</span><span class="p">)</span>

        <span class="c1"># Aggregate seed fragments from all Horovod workers</span>
        <span class="n">seed_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seed_fragment</span><span class="p">)</span>
        <span class="n">reduced_seed</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">seed_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;shuffle_seed&quot;</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">mpi_ops</span><span class="o">.</span><span class="n">Sum</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reduced_seed</span> <span class="o">%</span> <span class="n">max_rand</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We wrap the PyTorch optimizer with Horovod’s <code class="docutils literal notranslate"><span class="pre">DistributedOptimizer</span></code> class and scale the learning rate by the number of workers:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">lr_scaler</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We broadcast the model and optimizer parameters to all workers with Horovod:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>The rest of the script is the same as the MovieLens example in <a class="reference internal" href="../getting-started-movielens/03-Training-with-PyTorch.html"><span class="doc std std-doc">getting-started-movielens/03-Training-with-PyTorch.ipynb</span></a>. In order to run it with Horovod, we first need to write it to a file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%writefile &#39;./torch_trainer.py&#39;

import argparse
import glob
import os
from time import time

import cupy
import torch

import nvtabular as nvt
from nvtabular.framework_utils.torch.models import Model
from nvtabular.framework_utils.torch.utils import process_epoch
from nvtabular.loader.torch import DLDataLoader, TorchAsyncItr

# Horovod must be the last import to avoid conflicts
import horovod.torch as hvd  # noqa: E402, isort:skip


parser = argparse.ArgumentParser(description=&quot;Train a multi-gpu model with Torch and Horovod&quot;)
parser.add_argument(&quot;--dir_in&quot;, default=None, help=&quot;Input directory&quot;)
parser.add_argument(&quot;--batch_size&quot;, default=None, help=&quot;Batch size&quot;)
parser.add_argument(&quot;--cats&quot;, default=None, help=&quot;Categorical columns&quot;)
parser.add_argument(&quot;--cats_mh&quot;, default=None, help=&quot;Categorical multihot columns&quot;)
parser.add_argument(&quot;--conts&quot;, default=None, help=&quot;Continuous columns&quot;)
parser.add_argument(&quot;--labels&quot;, default=None, help=&quot;Label columns&quot;)
parser.add_argument(&quot;--epochs&quot;, default=1, help=&quot;Training epochs&quot;)
args = parser.parse_args()

hvd.init()

gpu_to_use = hvd.local_rank()

if torch.cuda.is_available():
    torch.cuda.set_device(gpu_to_use)


BASE_DIR = os.path.expanduser(args.dir_in or &quot;./data/&quot;)
BATCH_SIZE = int(args.batch_size or 16384)  # Batch Size
CATEGORICAL_COLUMNS = args.cats or [&quot;movieId&quot;, &quot;userId&quot;]  # Single-hot
CATEGORICAL_MH_COLUMNS = args.cats_mh or [&quot;genres&quot;]  # Multi-hot
NUMERIC_COLUMNS = args.conts or []

# Output from ETL-with-NVTabular
TRAIN_PATHS = sorted(glob.glob(os.path.join(BASE_DIR, &quot;train&quot;, &quot;*.parquet&quot;)))

proc = nvt.Workflow.load(os.path.join(BASE_DIR, &quot;workflow/&quot;))

EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)


# TensorItrDataset returns a single batch of x_cat, x_cont, y.
def collate_fn(x):
    return x


# Seed with system randomness (or a static seed)
cupy.random.seed(None)


def seed_fn():
    &quot;&quot;&quot;
    Generate consistent dataloader shuffle seeds across workers

    Reseeds each worker&#39;s dataloader each epoch to get fresh a shuffle
    that&#39;s consistent across workers.
    &quot;&quot;&quot;

    max_rand = torch.iinfo(torch.int).max // hvd.size()

    # Generate a seed fragment
    seed_fragment = cupy.random.randint(0, max_rand)

    # Aggregate seed fragments from all Horovod workers
    seed_tensor = torch.tensor(seed_fragment)
    reduced_seed = hvd.allreduce(seed_tensor, name=&quot;shuffle_seed&quot;, op=hvd.mpi_ops.Sum)

    return reduced_seed % max_rand


train_dataset = TorchAsyncItr(
    nvt.Dataset(TRAIN_PATHS),
    batch_size=BATCH_SIZE,
    cats=CATEGORICAL_COLUMNS + CATEGORICAL_MH_COLUMNS,
    conts=NUMERIC_COLUMNS,
    labels=[&quot;rating&quot;],
    device=gpu_to_use,
    global_size=hvd.size(),
    global_rank=hvd.rank(),
    shuffle=True,
    seed_fn=seed_fn,
)
train_loader = DLDataLoader(
    train_dataset, batch_size=None, collate_fn=collate_fn, pin_memory=False, num_workers=0
)


if isinstance(EMBEDDING_TABLE_SHAPES, tuple):
    EMBEDDING_TABLE_SHAPES_TUPLE = (
        {
            CATEGORICAL_COLUMNS[0]: EMBEDDING_TABLE_SHAPES[0][CATEGORICAL_COLUMNS[0]],
            CATEGORICAL_COLUMNS[1]: EMBEDDING_TABLE_SHAPES[0][CATEGORICAL_COLUMNS[1]],
        },
        {CATEGORICAL_MH_COLUMNS[0]: EMBEDDING_TABLE_SHAPES[1][CATEGORICAL_MH_COLUMNS[0]]},
    )
else:
    EMBEDDING_TABLE_SHAPES_TUPLE = (
        {
            CATEGORICAL_COLUMNS[0]: EMBEDDING_TABLE_SHAPES[CATEGORICAL_COLUMNS[0]],
            CATEGORICAL_COLUMNS[1]: EMBEDDING_TABLE_SHAPES[CATEGORICAL_COLUMNS[1]],
        },
        {CATEGORICAL_MH_COLUMNS[0]: EMBEDDING_TABLE_SHAPES[CATEGORICAL_MH_COLUMNS[0]]},
    )

model = Model(
    embedding_table_shapes=EMBEDDING_TABLE_SHAPES_TUPLE,
    num_continuous=0,
    emb_dropout=0.0,
    layer_hidden_dims=[128, 128, 128],
    layer_dropout_rates=[0.0, 0.0, 0.0],
).cuda()

lr_scaler = hvd.size()

optimizer = torch.optim.Adam(model.parameters(), lr=0.01 * lr_scaler)

hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)

optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())

for epoch in range(args.epochs):
    start = time()
    print(f&quot;Training epoch {epoch}&quot;)
    train_loss, y_pred, y = process_epoch(train_loader, model, train=True, optimizer=optimizer)
    hvd.join(gpu_to_use)
    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
    print(f&quot;Epoch {epoch:02d}. Train loss: {train_loss:.4f}.&quot;)
    hvd.join(gpu_to_use)
    t_final = time() - start
    total_rows = train_dataset.num_rows_processed
    print(
        f&quot;run_time: {t_final} - rows: {total_rows} - &quot;
        f&quot;epochs: {epoch} - dl_thru: {total_rows / t_final}&quot;
    )


hvd.join(gpu_to_use)
if hvd.local_rank() == 0:
    print(&quot;Training complete&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Overwriting ./torch_trainer.py
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!horovodrun -np 2 python torch_trainer.py --dir_in $BASE_DIR --batch_size 16384
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1,0]&lt;stdout&gt;:Training epoch 0
[1,1]&lt;stdout&gt;:Training epoch 0
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
_GatheringFuture exception was never retrieved
future: &lt;_GatheringFuture finished exception=CancelledError()&gt;
asyncio.exceptions.CancelledError
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1,1]&lt;stdout&gt;:Total batches: 488
[1,1]&lt;stderr&gt;:[2021-12-14 10:18:27.983077: E /tmp/pip-install-mwp6l21a/horovod_1f03263b83654efeb4c82a546f8aadfa/horovod/common/operations.cc:649] [1]: Horovod background loop uncaught exception: CUDA error: invalid configuration argument
[1,1]&lt;stderr&gt;:CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
[1,1]&lt;stderr&gt;:For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[1,1]&lt;stderr&gt;:Exception raised from launch_vectorized_kernel at /opt/pytorch/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh:103 (most recent call first):
[1,1]&lt;stderr&gt;:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x6c (0x7f07b599fe1c in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)
[1,1]&lt;stderr&gt;:frame #1: void at::native::gpu_kernel_impl&lt;at::native::FillFunctor&lt;float&gt; &gt;(at::TensorIteratorBase&amp;, at::native::FillFunctor&lt;float&gt; const&amp;) + 0xcfc (0x7f07b75db72c in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #2: void at::native::gpu_kernel&lt;at::native::FillFunctor&lt;float&gt; &gt;(at::TensorIteratorBase&amp;, at::native::FillFunctor&lt;float&gt; const&amp;) + 0x33b (0x7f07b75dc12b in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #3: &lt;unknown function&gt; + 0x1b9dbd3 (0x7f07b75c8bd3 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #4: at::native::fill_kernel_cuda(at::TensorIterator&amp;, c10::Scalar const&amp;) + 0x34 (0x7f07b75c9aa4 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #5: &lt;unknown function&gt; + 0x143a915 (0x7f0801bd8915 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #6: &lt;unknown function&gt; + 0x1029a89 (0x7f07b6a54a89 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #7: at::_ops::fill__Scalar::call(at::Tensor&amp;, c10::Scalar const&amp;) + 0x131 (0x7f08020e4611 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #8: at::native::zero_(at::Tensor&amp;) + 0x7f (0x7f0801bd804f in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #9: &lt;unknown function&gt; + 0x1037095 (0x7f07b6a62095 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
[1,1]&lt;stderr&gt;:frame #10: at::_ops::zero_::call(at::Tensor&amp;) + 0x12a (0x7f08020dd4ca in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #11: at::native::zeros(c10::ArrayRef&lt;long&gt;, c10::optional&lt;c10::ScalarType&gt;, c10::optional&lt;c10::Layout&gt;, c10::optional&lt;c10::Device&gt;, c10::optional&lt;bool&gt;) + 0x130 (0x7f0801df79b0 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #12: &lt;unknown function&gt; + 0x20089c9 (0x7f08027a69c9 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #13: &lt;unknown function&gt; + 0x1e00d5e (0x7f080259ed5e in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #14: &lt;unknown function&gt; + 0x1ded287 (0x7f080258b287 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #15: at::_ops::zeros::call(c10::ArrayRef&lt;long&gt;, c10::optional&lt;c10::ScalarType&gt;, c10::optional&lt;c10::Layout&gt;, c10::optional&lt;c10::Device&gt;, c10::optional&lt;bool&gt;) + 0x1ab (0x7f080211070b in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
[1,1]&lt;stderr&gt;:frame #16: horovod::torch::TorchOpContext::AllocateZeros(long, horovod::common::DataType, std::shared_ptr&lt;horovod::common::Tensor&gt;*) + 0xdf (0x7f0783bae9cf in /opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_lib_v2.cpython-38-x86_64-linux-gnu.so)
[1,1]&lt;stderr&gt;:frame #17: horovod::common::TensorQueue::GetTensorEntriesFromResponse(horovod::common::Response const&amp;, std::vector&lt;horovod::common::TensorTableEntry, std::allocator&lt;horovod::common::TensorTableEntry&gt; &gt;&amp;, bool) + 0x60d (0x7f0783b2da3d in /opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_lib_v2.cpython-38-x86_64-linux-gnu.so)
[1,1]&lt;stderr&gt;:frame #18: horovod::common::ResponseCache::put(horovod::common::Response const&amp;, horovod::common::TensorQueue&amp;, bool) + 0x1c6 (0x7f0783b21956 in /opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_lib_v2.cpython-38-x86_64-linux-gnu.so)
[1,1]&lt;stderr&gt;:frame #19: horovod::common::Controller::Co[1,1]&lt;stderr&gt;:mputeResponseList(bool, horovod::common::HorovodGlobalState&amp;, horovod::common::ProcessSet&amp;) + 0x1869 (0x7f0783ae2bd9 in /opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_lib_v2.cpython-38-x86_64-linux-gnu.so)
[1,1]&lt;stderr&gt;:frame #20: &lt;unknown function&gt; + 0x9cb3b (0x7f0783b05b3b in /opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_lib_v2.cpython-38-x86_64-linux-gnu.so)
[1,1]&lt;stderr&gt;:frame #21: &lt;unknown function&gt; + 0xcc9d4 (0x7f08d9efb9d4 in /opt/conda/bin/../lib/libstdc++.so.6)
[1,1]&lt;stderr&gt;:frame #22: &lt;unknown function&gt; + 0x9609 (0x7f09a71e1609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)
[1,1]&lt;stderr&gt;:frame #23: clone + 0x43 (0x7f09a6fa1293 in /usr/lib/x86_64-linux-gnu/libc.so.6)
[1,1]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:/opt/conda/lib/python3.8/site-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: <span class=" -Color -Color-Bold">Grid size (13) &lt; 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.</span>
[1,1]&lt;stderr&gt;:  warn(NumbaPerformanceWarning(msg))
[1,1]&lt;stderr&gt;:Traceback (most recent call last):
[1,1]&lt;stderr&gt;:  File &quot;/opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_ops.py&quot;, line 944, in synchronize
[1,1]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,1]&lt;stderr&gt;:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[1,1]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:During handling of the above exception, another exception occurred:
[1,1]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:Traceback (most recent call last):
[1,1]&lt;stderr&gt;:  File &quot;torch_trainer.py&quot;, line 138, in &lt;module&gt;
[1,1]&lt;stderr&gt;:    hvd.join(gpu_to_use)
[1,1]&lt;stderr&gt;:  File &quot;/opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_ops.py&quot;, line 972, in join
[1,1]&lt;stderr&gt;:    return synchronize(handle).item()
[1,1]&lt;stderr&gt;:  File &quot;/opt/conda/lib/python3.8/site-packages/horovod/torch/mpi_ops.py&quot;, line 949, in synchronize
[1,1]&lt;stderr&gt;:    raise HorovodInternalError(e)
[1,1]&lt;stderr&gt;:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
^C
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01-03-MultiGPU-Download-Convert-ETL-with-NVTabular-Training-with-TensorFlow.html" class="btn btn-neutral float-left" title="Multi-GPU with MovieLens: ETL and Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../multi-gpu-toy-example/multi-gpu_dask.html" class="btn btn-neutral float-right" title="Multi-GPU Scaling in NVTabular with Dask" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.11.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="01-03-MultiGPU-Download-Convert-ETL-with-NVTabular-Training-with-PyTorch.html">v0.11.0</a></dd>
      <dd><a href="../../../v1.0.0/index.html">v1.0.0</a></dd>
      <dd><a href="../../../v1.1.0/index.html">v1.1.0</a></dd>
      <dd><a href="../../../v1.1.1/index.html">v1.1.1</a></dd>
      <dd><a href="../../../v1.2.0/index.html">v1.2.0</a></dd>
      <dd><a href="../../../v1.2.1/index.html">v1.2.1</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>